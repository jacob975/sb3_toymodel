{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9043e13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.7.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "import os\n",
    "os.add_dll_directory('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5/bin')\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Activation, Dropout\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "\n",
    "# If you have more than 1 GPU, you might want to specify which GPU for training.\n",
    "# In this case, I have 2 GPU and the second one is RTX 2080ti, so I pick the `second` one.\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0' # The second\n",
    "#tf.config.set_soft_device_placement(True)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960dc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = '20211215_BipedalWalker-v3_SAC'\n",
    "env_name = 'BipedalWalker-v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b4df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_action = 4\n",
    "n_state_var = 24\n",
    "gamma = 0.996\n",
    "lr = 1e-3\n",
    "dr = 0.1 # dropout rate\n",
    "wv_dim = 64\n",
    "ff_dim = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88703d48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ba4a0",
   "metadata": {},
   "source": [
    "## Test environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e28332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Action ---\n",
      "Box([-1. -1. -1. -1.], [1. 1. 1. 1.], (4,), float32)\n",
      "\n",
      "--- Observation ---\n",
      "Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf], (24,), float32)\n"
     ]
    }
   ],
   "source": [
    "# The action domain\n",
    "print('-- Action ---')\n",
    "print(env.action_space)\n",
    "print('')\n",
    "# The observing domain\n",
    "print('--- Observation ---')\n",
    "print(env.observation_space)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e525f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.0033001 , -0.01897045, -0.0091202 ,  0.02070081, -0.29741445,\n",
       "        -0.6126805 ,  1.4828402 ,  0.99364257,  1.        ,  0.32402867,\n",
       "         0.85520095,  0.1335144 , -0.999922  ,  1.        ,  0.45091254,\n",
       "         0.45603332,  0.47199342,  0.5007652 ,  0.54633844,  0.6162627 ,\n",
       "         0.72539467,  0.90622747,  1.        ,  1.        ], dtype=float32),\n",
       " -0.04889316515003879,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af98b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "n_trial = 1\n",
    "n_action = 1000\n",
    "# Initialize a New Env\n",
    "for _ in range(n_trial):\n",
    "    if env:\n",
    "        env.close()\n",
    "    env = gym.make(env_name)\n",
    "    #env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    state = env.reset()\n",
    "    for _ in range(n_action):\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2217724",
   "metadata": {},
   "source": [
    "# Make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aedb7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kernel of value function and the actor( a.k.a. prediction function)\n",
    "def getKernel():\n",
    "    _input = Input((n_state_var,))\n",
    "    m = Dense(wv_dim)(_input)\n",
    "    for i in range(1):\n",
    "        tmp = Dense(ff_dim)(m)\n",
    "        tmp = Activation('relu')(tmp)\n",
    "        tmp = Dense(wv_dim)(tmp)\n",
    "        m = BatchNormalization(epsilon=1e-6)(tmp+m)\n",
    "        m = Activation('relu')(m)\n",
    "    model = Model(\n",
    "        _input,\n",
    "        m,\n",
    "        name = 'kernel',\n",
    "    ) \n",
    "    return model\n",
    "\n",
    "# Used to estimate winning percentage\n",
    "# s -> v\n",
    "def getValueFunction(kernal):\n",
    "    s_input = Input((n_state_var,))\n",
    "    m = kernel(s_input)\n",
    "    m = Dropout(dr)(m)\n",
    "    reward = Dense(1)(m)\n",
    "    model = Model(\n",
    "        s_input,\n",
    "        reward,\n",
    "        name = 'value_function'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Used to modeling the actions\n",
    "# s -> a\n",
    "def getActor(kernel):\n",
    "    s_input = Input((n_state_var,))\n",
    "    m = kernel(s_input)\n",
    "    output = Dense(n_action)(m)\n",
    "    output = Activation('sigmoid')(output)\n",
    "    model = Model(\n",
    "        s_input,\n",
    "        output,\n",
    "        name = 'actor',\n",
    "    ) \n",
    "    return model\n",
    "\n",
    "# Used to modeling immediate rewards\n",
    "# s,a -> r\n",
    "def getRewardFunction():\n",
    "    s_input = Input((n_state_var,))\n",
    "    s_emb = Dense(wv_dim)(s_input)\n",
    "    a_input = Input((n_action))\n",
    "    a_emb = Dense(wv_dim)(a_input)\n",
    "    m = BatchNormalization(epsilon=1e-6)(a_emb+s_emb)\n",
    "    for i in range(2):\n",
    "        tmp = Dense(ff_dim)(m)\n",
    "        tmp = Activation('relu')(tmp)\n",
    "        tmp = Dense(wv_dim)(tmp)\n",
    "        m = BatchNormalization(epsilon=1e-6)(m+tmp)\n",
    "        m = Activation('relu')(m)\n",
    "    m = Dropout(dr)(m)\n",
    "    output = Dense(1)(m)\n",
    "    model = Model(\n",
    "        [s_input, a_input],\n",
    "        output,\n",
    "        name = 'reward_function',\n",
    "    )\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68bd5072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"value_function\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 24)]              0         \n",
      "                                                                 \n",
      " kernel (Functional)         (None, 64)                266112    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,177\n",
      "Trainable params: 266,049\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Model: \"reward_function\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 64)           64064       ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64)           1600        ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 64)          0           ['dense_9[0][0]',                \n",
      " mbda)                                                            'dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64)          256         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 2048)         133120      ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 2048)         0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           131136      ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 64)          0           ['batch_normalization_2[0][0]',  \n",
      " mbda)                                                            'dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64)          256         ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 64)           0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 2048)         133120      ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 2048)         0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 64)           131136      ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 64)          0           ['activation_6[0][0]',           \n",
      " mbda)                                                            'dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64)          256         ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 64)           0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1)            65          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 595,009\n",
      "Trainable params: 594,625\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 24)]              0         \n",
      "                                                                 \n",
      " kernel (Functional)         (None, 64)                266112    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              65000     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1000)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 331,112\n",
      "Trainable params: 330,984\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kernel = getKernel()\n",
    "actor  = getActor(kernel)\n",
    "# Not to share the kernel\n",
    "kernel = getKernel()\n",
    "v_func = getValueFunction(kernel)\n",
    "r_func = getRewardFunction()\n",
    "v_func.summary()\n",
    "r_func.summary()\n",
    "actor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19fc1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_value(r_arr, gamma = gamma): # 0.996\n",
    "    #print(r_arr)\n",
    "    decay_arr = np.array([gamma**i for i in range(len(r_arr)+1)])\n",
    "    #print(decay_arr)\n",
    "    q_arr = [sum(r_arr[i:] * decay_arr[:-i-1]) for i in range(len(r_arr))]\n",
    "    return q_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e8da0",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b21845",
   "metadata": {},
   "source": [
    "## Value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4019569",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_func_optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "v_loss_object = tf.keras.losses.MeanSquaredError(reduction = 'none')\n",
    "entropy = tf.keras.losses.BinaryCrossentropy(reduction = 'none')\n",
    "\n",
    "v_func.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['mean_squared_error'],\n",
    ")\n",
    "\n",
    "# The loss for policy gradient with partition function\n",
    "def v_func_loss(pred, policy_a, r):\n",
    "    loss = 0.5 * (pred - r - entropy())**2\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function()\n",
    "def train_v_step(s_in, policy_a, r):\n",
    "    pred = None\n",
    "    loss = None\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = v_func(s_in)\n",
    "        loss = v_func_loss(pred, policy_a, r)\n",
    "    gradients = tape.gradient(loss, v_func.trainable_variables)    \n",
    "    v_func_optimizer.apply_gradients(zip(gradients, v_func.trainable_variables))    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82b4ce",
   "metadata": {},
   "source": [
    "## Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c4df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_func_optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "r_loss_object = tf.keras.losses.MeanSquaredError(reduction = 'none')\n",
    "\n",
    "r_func.compile(\n",
    "    loss = 'mean_squared_error',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['mean_squared_error'],\n",
    ")\n",
    "\n",
    "# The loss for policy gradient with partition function\n",
    "def r_func_loss(pred, v1, reward):\n",
    "    loss = 0.5 * (pred - gamma*v1 - reward)**2\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function()\n",
    "def train_r_step(s_in, a_in, v1, reward):\n",
    "    pred = None\n",
    "    loss = None\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = r_func([s_in, a_in])\n",
    "        loss = r_func_loss(pred, v1, reward)\n",
    "    gradients = tape.gradient(loss, r_func.trainable_variables)    \n",
    "    r_func_optimizer.apply_gradients(zip(gradients, r_func.trainable_variables))    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1f6bc",
   "metadata": {},
   "source": [
    "## Actor(Predition function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6223df",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(reduction = 'none')\n",
    "\n",
    "actor.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(),\n",
    "    metrics=['binary_crossentropy'],\n",
    ")\n",
    "\n",
    "# The loss for policy gradient with partition function\n",
    "def actor_loss(real, pred, r, beta = 1.0):\n",
    "    loss = loss_object(real, pred)\n",
    "    loss = loss * r - beta * entropy(pred, pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function()\n",
    "def train_a_step(s_in, real, r):\n",
    "    pred = None\n",
    "    loss = None\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = actor(s_in)\n",
    "        loss = actor_loss(real, pred, r)\n",
    "    gradients = tape.gradient(loss, actor.trainable_variables)    \n",
    "    actor_optimizer.apply_gradients(zip(gradients, actor.trainable_variables))    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b299a02",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc2c13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_buffer():\n",
    "    def __init__(self, n_replay_buffer):\n",
    "        self.n_replay_buffer = n_replay_buffer\n",
    "        self.holder = []\n",
    "        return\n",
    "    # Put new trajectories into the replay buffer\n",
    "    def stack(self, _input):\n",
    "        self.holder.extend(list(_input))\n",
    "        self.holder = self.holder[-self.n_replay_buffer:]\n",
    "        return\n",
    "    # Take some of trajectories from the replay buffer\n",
    "    def sample(self, q_len, idx = None):\n",
    "        if q_len > len(self.holder):\n",
    "            return self.holder\n",
    "        elif isinstance(idx, type(None)):\n",
    "            idx = np.arange(self.n_replay_buffer)\n",
    "            np.random.shuffle(idx)\n",
    "            return [self.holder[i] for i in idx[:q_len]]\n",
    "        else:\n",
    "            return [self.holder[i] for i in idx[:q_len]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73885ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_rb = replay_buffer(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f87b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 1., 1.]), array([1., 1., 1.]), array([1., 1., 1.])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_rb.stack(np.ones((3,3)))\n",
    "example_rb.holder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d59871",
   "metadata": {},
   "source": [
    "# Entire training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed817f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Initialization\n",
    "n_batch = 1000\n",
    "n_trial = 32\n",
    "n_step = 1000\n",
    "n_replay_buffer = 102400\n",
    "play_size = 10240\n",
    "\n",
    "# List for recording losses\n",
    "v_loss_list = []\n",
    "a_loss_list = []\n",
    "r_loss_list = []\n",
    "score_list = []\n",
    "best_score = None\n",
    "\n",
    "# Replay buffer\n",
    "s_replay_buffer = replay_buffer(n_replay_buffer)\n",
    "s1_replay_buffer = replay_buffer(n_replay_buffer)\n",
    "a_replay_buffer = replay_buffer(n_replay_buffer)\n",
    "r_replay_buffer = replay_buffer(n_replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad9c93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(idx):\n",
    "    env = gym.make(env_name)\n",
    "    #env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    state = env.reset()\n",
    "    state_list = []\n",
    "    state_list.append(state)\n",
    "    s1_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    kernel = getKernel()\n",
    "    actor  = getActor(kernel)\n",
    "    try:\n",
    "        actor.load_weights(f'{folder_name}/actor_current.h5')\n",
    "    except:\n",
    "        pass\n",
    "    #-----------------------------------\n",
    "    # The Game Section\n",
    "    for i in range(n_step):\n",
    "        action = actor(np.array([state])).numpy()\n",
    "        action = action[0]*2 - 1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        s1_list.append(state)\n",
    "        action_list.append(action)\n",
    "        #-------------------------------\n",
    "        # Not to use -100 reward\n",
    "        if reward == -100:\n",
    "            reward_list.append(-0.03)\n",
    "        else:\n",
    "            reward_list.append(reward)\n",
    "        #-------------------------------\n",
    "        if done:\n",
    "            break\n",
    "        # Don't save the last env if the game is not done at the end\n",
    "        elif i == n_step -1:\n",
    "            break\n",
    "        else:\n",
    "            state_list.append(state)\n",
    "    env.close()\n",
    "    return state_list, s1_list, action_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90de3aea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "Batch: 1, Score: -0.0300, A loss: -0.4013, V loss: 1.0456, R loss: 1.3520, elapsed time: 8 secs\n",
      "Batch: 2, Score: -0.0300, A loss: -1.3729, V loss: 0.4028, R loss: 0.0343, elapsed time: 6 secs\n",
      "Batch: 3, Score: -0.0300, A loss: -1.9334, V loss: 1.5397, R loss: 0.2351, elapsed time: 7 secs\n",
      "Batch: 4, Score: -0.0300, A loss: -1.8029, V loss: 0.7824, R loss: 0.1066, elapsed time: 9 secs\n",
      "Batch: 5, Score: -0.0300, A loss: -1.5782, V loss: 0.1764, R loss: 0.1637, elapsed time: 9 secs\n",
      "Batch: 6, Score: -0.0300, A loss: -1.4721, V loss: 0.0368, R loss: 0.2029, elapsed time: 8 secs\n",
      "Batch: 7, Score: -0.1169, A loss: -1.4571, V loss: 0.0321, R loss: 0.1778, elapsed time: 16 secs\n",
      "Batch: 8, Score: -0.0300, A loss: -1.4904, V loss: 0.0126, R loss: 0.1672, elapsed time: 9 secs\n",
      "Batch: 9, Score: -0.0300, A loss: -1.6564, V loss: 0.0116, R loss: 0.1336, elapsed time: 8 secs\n",
      "Batch: 10, Score: -0.0300, A loss: -1.8937, V loss: 0.0353, R loss: 0.1038, elapsed time: 11 secs\n",
      "Batch: 11, Score: -0.0300, A loss: -2.1915, V loss: 0.1331, R loss: 0.1233, elapsed time: 8 secs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17868/920243765.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#    action_array.extend(action_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#    reward_array.extend(reward_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mstate_holder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms1_holder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_holder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_holder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhelper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_trial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;31m#-----------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Trajectory holder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.2544.0_x64__qbz5n2kfra8p0\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.2544.0_x64__qbz5n2kfra8p0\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "st = time.time()\n",
    "for b in range(n_batch):\n",
    "    # Clear Previous Behaviors\n",
    "    state_array = []\n",
    "    s1_array = []\n",
    "    reward_array = []\n",
    "    action_array = []\n",
    "    # The model for parallel\n",
    "    actor.save(f'{folder_name}/actor_current.h5')\n",
    "    # Multiprocessing\n",
    "    #for idx in range(n_trial):\n",
    "    #    state_list, s1_list, action_list, reward_list = helper(idx)\n",
    "    #    state_array.extend(state_list)\n",
    "    #    s1_array.extend(s1_list)\n",
    "    #    action_array.extend(action_list)\n",
    "    #    reward_array.extend(reward_list)\n",
    "    state_holder, s1_holder, action_holder, reward_holder = zip(*Parallel(n_jobs = 8)(delayed(helper)(i) for i in range(n_trial)))\n",
    "    #-----------------------------------\n",
    "    # Trajectory holder\n",
    "    for i in range(n_trial):\n",
    "        state_array.extend(state_holder[i])\n",
    "        s1_array.extend(s1_holder[i])\n",
    "        action_array.extend(action_holder[i])\n",
    "        reward_array.extend(reward_holder[i])\n",
    "    score = np.mean([r[-1] for r in reward_holder])\n",
    "    #score = np.mean(reward_array)\n",
    "    score_list.append(score)\n",
    "    if isinstance(best_score, type(None)) or (score > best_score):\n",
    "        best_score = score\n",
    "        v_func.save(f'{folder_name}/v_func.h5')\n",
    "        r_func.save(f'{folder_name}/r_func.h5')\n",
    "        actor.save(f'{folder_name}/actor.h5')\n",
    "        print('Model saved')\n",
    "    \n",
    "    #---------------------------------\n",
    "    # Save to replay buffer\n",
    "    s_replay_buffer.stack( state_array)\n",
    "    s1_replay_buffer.stack(s1_array)\n",
    "    a_replay_buffer.stack( action_array)\n",
    "    r_replay_buffer.stack( reward_array)\n",
    "    \n",
    "    buffer_idx = np.arange(len(s_replay_buffer.holder))\n",
    "    np.random.shuffle(buffer_idx)\n",
    "    \n",
    "    state_array = np.array(s_replay_buffer.sample(play_size, buffer_idx), dtype = np.float32)\n",
    "    s1_array = np.array(s1_replay_buffer.sample(play_size, buffer_idx), dtype = np.float32)\n",
    "    action_array = np.array(a_replay_buffer.sample(play_size, buffer_idx), dtype = np.float32)\n",
    "    reward_array = np.array(r_replay_buffer.sample(play_size, buffer_idx), dtype = np.float32)\n",
    "    #---------------------------------\n",
    "    # Update Value function\n",
    "    fake_reward_array = r_func.predict([state_array, action_array])\n",
    "    #p_action_array = actor.predict(state_array)\n",
    "    #onehot_action_array = tf.one_hot(action_array, depth = n_action)\n",
    "    #policy_array = tf.reduce_sum(p_action_array * onehot_action_array, axis = -1)\n",
    "    policy_array = actor.predict(state_array)\n",
    "    v_loss = train_v_step(state_array, policy_array, fake_reward_array)\n",
    "    v_loss_list.append(v_loss)\n",
    "    # Update Reward function\n",
    "    v1_array = v_func.predict(s1_array)\n",
    "    r_loss = train_r_step(state_array, action_array, v1_array, reward_array)\n",
    "    r_loss_list.append(r_loss)\n",
    "    # Update Actor\n",
    "    a_loss = train_a_step(state_array, action_array, fake_reward_array)\n",
    "    a_loss_list.append(a_loss)\n",
    "    #---------------------------------\n",
    "    # Report\n",
    "    elapsed_time = time.time() - st\n",
    "    st = time.time()\n",
    "    print(\n",
    "        f'Batch: {b+1}, '\n",
    "        f'Score: {score:.4f}, '\n",
    "        f'A loss: {a_loss:.4f}, '\n",
    "        f'V loss: {v_loss:.4f}, '\n",
    "        f'R loss: {r_loss:.4f}, '\n",
    "        f'elapsed time: {elapsed_time:.0f} secs'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c02ce6",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc5c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actor.load_weights(f'{folder_name}/actor.h5')\n",
    "v_func.load_weights(f'{folder_name}/v_func.h5')\n",
    "r_func.load_weights(f'{folder_name}/r_func.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "st = time.time()\n",
    "n_trial = 1000\n",
    "n_step = 1000\n",
    "for i in range(n_trial):\n",
    "    # Initialize a New Env\n",
    "    if env:\n",
    "        env.close()\n",
    "    env = gym.make(env_name)\n",
    "    env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    state = env.reset()\n",
    "    for _ in range(n_step):\n",
    "        action = actor(np.array([state])).numpy()\n",
    "        action = action[0]*2 - 1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5cc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
